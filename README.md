
# Confusion Matrix and Model Evaluation Tool

This Streamlit application allows users to input confusion matrix data for multiple models, calculates various evaluation metrics (accuracy, precision, recall, F1 score), and provides visualizations and downloadable Excel reports. It also highlights potential overfitting by comparing train and test metrics.

## Features

- Input confusion matrix data for multiple models.
- Calculate evaluation metrics: accuracy, precision, recall, and F1 score.
- Detect overfitting by comparing train and test metrics.
- Generate a downloadable Excel report with color-coded best and worst scores.
- Provide a recommended model based on the highest F1 score.

## Installation

1. Clone the repository:
    ```sh
    git clone https://github.com/yourusername/confusion-matrix-evaluation.git
    ```
2. Navigate to the project directory:
    ```sh
    cd confusion-matrix-evaluation
    ```
3. Install the required dependencies:
    ```sh
    pip install -r requirements.txt
    ```

## Usage

1. Run the application:
    ```sh
    streamlit run cf.py
    ```
2. Open your browser and navigate to the provided URL to interact with the application.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact

M.R. Vijay Krishnan  
vijaykrishnanmr@gmail.com
